{"cells": [{"cell_type": "code", "execution_count": 3, "id": "e8f9741b-a4ff-46ed-99ad-47fe023a99cc", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "lastException = null\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "null"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Row\nimport org.apache.hadoop.fs.{FileSystem, Path}"}, {"cell_type": "code", "execution_count": 4, "id": "8d10bace-3a18-4fd6-b9d3-f061668d4269", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "spark = org.apache.spark.sql.SparkSession@1d911e19\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "org.apache.spark.sql.SparkSession@1d911e19"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "val spark = SparkSession.builder()\n  .appName(\"CaseStudy5 - Time-Based Data Partitioning for Ratings\")\n  .master(\"local[*]\") // Use appropriate cluster manager in production\n  .getOrCreate()"}, {"cell_type": "code", "execution_count": 5, "id": "0a281adf-a61b-49bb-ab49-54d82ba79ec3", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "ratingsDffPath = gs://artifacts_spark_jobs/rating.csv\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "gs://artifacts_spark_jobs/rating.csv"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "val ratingsDffPath = \"gs://artifacts_spark_jobs/rating.csv\""}, {"cell_type": "code", "execution_count": 7, "id": "bf5cfe5b-9ada-4eb8-8abf-7416754b804c", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "ratingsDFF = [userId: int, movieId: int ... 2 more fields]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[userId: int, movieId: int ... 2 more fields]"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "val ratingsDFF = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(ratingsDffPath)"}, {"cell_type": "code", "execution_count": 8, "id": "8131e658-dea9-4b2c-8e0c-4a964e6bdbff", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "updatedDF = [userId: int, movieId: int ... 3 more fields]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[userId: int, movieId: int ... 3 more fields]"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "val updatedDF = ratingsDFF.withColumn(\"year\", year(col(\"timestamp\")))"}, {"cell_type": "code", "execution_count": 9, "id": "684fec18-4f40-4a34-8a16-eb009d7b1023", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "trimmedDF = [userId: int, movieId: int ... 3 more fields]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[userId: int, movieId: int ... 3 more fields]"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "// Has lot of records, so taking only 100000 records\nval trimmedDF = updatedDF.limit(100000)"}, {"cell_type": "code", "execution_count": null, "id": "7dceef1d-4edc-43c2-ab85-6292c91d6e0a", "metadata": {}, "outputs": [], "source": "val hdfsPath = \"hdfs:/user/hdfs/CaseStudies\"\ntrimmedDF.coalesce(1).write.partitionBy(\"year\").format(\"parquet\").mode(\"overwrite\").save(hdfsPath)"}], "metadata": {"kernelspec": {"display_name": "Apache Toree - Scala", "language": "scala", "name": "apache_toree_scala"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "2.12.15"}}, "nbformat": 4, "nbformat_minor": 5}