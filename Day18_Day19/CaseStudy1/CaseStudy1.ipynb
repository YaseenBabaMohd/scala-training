{"cells": [{"cell_type": "code", "execution_count": null, "id": "d4468f52-0ae0-4b42-832a-e810fb1f3b03", "metadata": {"tags": []}, "outputs": [], "source": "import org.apache.spark.sql.{SparkSession, DataFrame}\nimport org.apache.spark.sql.functions._"}, {"cell_type": "code", "execution_count": 1, "id": "3fa7be60-5f5d-4bdb-86f7-8eadc68396b5", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "spark = org.apache.spark.sql.SparkSession@1176d7a0\nsparkContext = org.apache.spark.SparkContext@6e19d0c0\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "org.apache.spark.SparkContext@6e19d0c0"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "val spark = SparkSession.builder()\n      .appName(\"Broadcast Join Example\")\n      .master(\"local[*]\") // Use local mode for simplicity\n      .getOrCreate()\nval sparkContext = spark.sparkContext"}, {"cell_type": "code", "execution_count": 2, "id": "436ec0d2-8478-4e75-b3f6-7666ce3a504f", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "transactionsLocation = gs://artifacts_spark_jobs/movie_metadata/yaseen/transaction_logs.csv\nusersData = gs://artifacts_spark_jobs/movie_metadata/yaseen/user_details.csv\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "gs://artifacts_spark_jobs/movie_metadata/yaseen/user_details.csv"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "val transactionsLocation = \"gs://artifacts_spark_jobs/movie_metadata/yaseen/transaction_logs.csv\"\nval usersData = \"gs://artifacts_spark_jobs/movie_metadata/yaseen/user_details.csv\""}, {"cell_type": "code", "execution_count": 3, "id": "ada2ebc0-60c9-4547-b643-5c90007f0b93", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "transactionsData = [user_id: int, transaction_type: string ... 1 more field]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[user_id: int, transaction_type: string ... 1 more field]"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "val transactionsData = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(transactionsLocation)"}, {"cell_type": "code", "execution_count": 4, "id": "fe3aac3c-c968-4e26-a7ad-76ed798991be", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+----------------+------+\n|user_id|transaction_type|amount|\n+-------+----------------+------+\n|    623|          Refund|357.42|\n|    393|        Purchase|518.77|\n|    702|        Purchase|980.64|\n|    829|        Purchase|107.04|\n|     39|        Purchase|736.92|\n|    394|          Refund|660.84|\n|    376|        Purchase|782.27|\n|    125|          Refund| 438.5|\n|    257|          Refund|614.49|\n|    966|        Purchase|986.76|\n|    278|        Purchase|506.34|\n|    918|        Purchase|903.83|\n|    462|        Purchase| 50.09|\n|    581|        Purchase|744.69|\n|    948|        Purchase|224.01|\n|    612|          Refund|588.98|\n|    598|        Purchase|700.02|\n|    836|          Refund|549.84|\n|    682|          Refund|881.29|\n|    733|          Refund|943.92|\n+-------+----------------+------+\nonly showing top 20 rows\n\n"}], "source": "transactionsData.show()"}, {"cell_type": "code", "execution_count": 5, "id": "b2cb53f5-8622-44a9-9da2-b210906f3570", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "userData = [user_id: int, name: string]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[user_id: int, name: string]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "val userData = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(usersData)"}, {"cell_type": "code", "execution_count": 6, "id": "49856751-72f6-4c09-9da8-433cb6fa1634", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+-------+\n|user_id|   name|\n+-------+-------+\n|      1| User_1|\n|      2| User_2|\n|      3| User_3|\n|      4| User_4|\n|      5| User_5|\n|      6| User_6|\n|      7| User_7|\n|      8| User_8|\n|      9| User_9|\n|     10|User_10|\n|     11|User_11|\n|     12|User_12|\n|     13|User_13|\n|     14|User_14|\n|     15|User_15|\n|     16|User_16|\n|     17|User_17|\n|     18|User_18|\n|     19|User_19|\n|     20|User_20|\n+-------+-------+\nonly showing top 20 rows\n\n"}], "source": "userData.show()"}, {"cell_type": "code", "execution_count": 9, "id": "061b6f02-ae01-40f4-a8da-38b80a0185a3", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "userDF = [user_id: int, name: string]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[user_id: int, name: string]"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "val userDF = userData.toDF"}, {"cell_type": "code", "execution_count": 10, "id": "352ce087-cad2-4374-b16f-5c449183c206", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "transactionsDF = [user_id: int, transaction_type: string ... 1 more field]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[user_id: int, transaction_type: string ... 1 more field]"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "val transactionsDF = transactionsData.toDF"}, {"cell_type": "code", "execution_count": 12, "id": "3dd6edd2-a28e-438c-a7b4-7446b01a0597", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "joinedData = [user_id: int, transaction_type: string ... 2 more fields]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[user_id: int, transaction_type: string ... 2 more fields]"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "import org.apache.spark.sql.functions.broadcast \nval joinedData = transactionsDF.join(\n      broadcast(userDF), // Broadcast the smaller dataset\n      Seq(\"user_id\"),\n      \"inner\" // Perform an inner join\n    )"}, {"cell_type": "code", "execution_count": 13, "id": "cd19aef9-fa7c-4eb0-8717-725b89bfff35", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+----------------+------+--------+\n|user_id|transaction_type|amount|    name|\n+-------+----------------+------+--------+\n|    623|          Refund|357.42|User_623|\n|    393|        Purchase|518.77|User_393|\n|    702|        Purchase|980.64|User_702|\n|    829|        Purchase|107.04|User_829|\n|     39|        Purchase|736.92| User_39|\n|    394|          Refund|660.84|User_394|\n|    376|        Purchase|782.27|User_376|\n|    125|          Refund| 438.5|User_125|\n|    257|          Refund|614.49|User_257|\n|    966|        Purchase|986.76|User_966|\n|    278|        Purchase|506.34|User_278|\n|    918|        Purchase|903.83|User_918|\n|    462|        Purchase| 50.09|User_462|\n|    581|        Purchase|744.69|User_581|\n|    948|        Purchase|224.01|User_948|\n|    612|          Refund|588.98|User_612|\n|    598|        Purchase|700.02|User_598|\n|    836|          Refund|549.84|User_836|\n|    682|          Refund|881.29|User_682|\n|    733|          Refund|943.92|User_733|\n+-------+----------------+------+--------+\nonly showing top 20 rows\n\n"}], "source": "joinedData.show"}, {"cell_type": "code", "execution_count": 14, "id": "b93b279f-2b5b-4f6d-ad73-bbcd1c86a80c", "metadata": {"tags": []}, "outputs": [], "source": "spark.stop"}, {"cell_type": "code", "execution_count": null, "id": "8fbc32f1-2327-4775-a9eb-0e5dc8d41cfd", "metadata": {}, "outputs": [], "source": "// Without Broadcasting: A standard join in Spark requires shuffling, where data from both datasets is exchanged across the network to \n// co-locate matching keys. This process involves moving large amounts of data between worker nodes, \n// which is both time-consuming and resource-intensive.\n\n\n// With Broadcasting: The small dataset is sent to all worker nodes. \n// Each worker can perform the join locally, eliminating the need for shuffling the large dataset."}], "metadata": {"kernelspec": {"display_name": "Apache Toree - Scala", "language": "scala", "name": "apache_toree_scala"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "2.12.15"}}, "nbformat": 4, "nbformat_minor": 5}